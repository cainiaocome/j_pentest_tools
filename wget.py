#!/usr/bin/env python
# encoding: utf-8

import os
import re
import time
import random
import traceback
import requests
import sys
import argparse
import threading
import json
import logging
from pprint import pprint, pformat

logging.getLogger('requests').setLevel(logging.WARNING)
logging.basicConfig(format='%(asctime)s %(levelname)s %(message)s', level=logging.INFO)

ua = 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'
block_size = 1024*256
worker_thread_count = 128
timeout = 7
proxies = None

s = requests.session()
s.headers.update( {'User-Agent': ua} )

def log(msg):
    logging.info( msg + '\n' )
    logging.info( '-'*30 )

def get_real_url( url ):
    real_url = url
    while True:
        if proxies:
            r = s.get( real_url, timeout=timeout, stream=True, proxies=proxies, verify=False )
        else:
            r = s.get( real_url, timeout=timeout, stream=True )
        try:
            real_url = r.headers['location']
            logging.info( r.status_code )
            logging.info( 'url redirect to:{}'.format( real_url ) )
        except:
            break
    return real_url

def support_continue(url):
    headers = {
        'Range': 'bytes=0-4'
    }
    try:
        if proxies:
            r = s.get(url, headers = headers, timeout=timeout, stream=True, proxies=proxies )
        else:
            r = s.get(url, headers = headers, timeout=timeout, stream=True)
        r.close()
        log( 'support_continue' )
        log( pformat( r.status_code ) )
        log( pformat( dict(r.headers) ) )
        log( 'support_continue' )
        if 'Accept-Ranges' in dict(r.headers).keys():
            return True
        crange = r.headers['content-range']
        return True
    except:
        log( traceback.format_exc() )
        sys.exc_clear()
    return False

def get_content_length(url):
    headers = {
        'Range': 'bytes=0-4'
    }
    try:
        if proxies:
            r = s.get(url, headers = headers, timeout=timeout, stream=True, proxies=proxies, verify=False )
        else:
            r = s.get(url, headers = headers, timeout=timeout, stream=True )
        r.close()
        log( 'get_content_length' )
        log( pformat( r.status_code ) )
        log( pformat( dict(r.headers) ) )
        log( 'get_content_length' )
        crange = r.headers['content-range']
        total = int(re.match(ur'^bytes 0-4/(\d+)$', crange).group(1))
        return total
    except:
        log( traceback.format_exc() )
        sys.exc_clear()
    try:
        total = int(r.headers['content-length'])
    except:
        sys.exc_clear()
        total = 0
    return total

def partial_get( url, start, end ):
    headers = {
        'Range': 'bytes={}-{}'.format( start, end )
    }
    if proxies:
        r = s.get(url, headers = headers, timeout=timeout, proxies=proxies, verify=False )
    else:
        r = s.get(url, headers = headers, timeout=timeout)
    r.close()
    return r.content

def wget( args ):
    real_url = get_real_url( args.url )
    args.url = real_url
    if not get_content_length( args.url ):
        logging.info( 'url total length 0, exit...' )
        return
    if not support_continue( args.url ):
        logging.info( 'url do not support_continue, exit...' )
        return

    block_list = []
    block_list_write_lock = threading.Lock()
    # get_info
    while True:
        try:
            url_content_length = get_content_length( args.url )
            log( 'url_content_length:{} bytes'.format( url_content_length ) )
            block_count = url_content_length / block_size
            if url_content_length % block_size > 0:
                block_count = block_count + 1
            log( 'block_count:{}'.format( block_count ) )
            map( lambda x:block_list.append('to_get'), range(block_count) )
            break
        except:
            log( traceback.format_exc() )
            sys.exc_clear()
    time.sleep( 3 )

    def check_finished():
        remain_job_list = filter(lambda x:True if x=='to_get' or x=='getting' else False, block_list)
        print 'remain_job_list len:', len(remain_job_list)
        if len(remain_job_list)==0:
            return True
        else:
            return False

    def get_getting_index_list():
        getting_index_list = []
        for index in range(len(block_list)):
            if block_list[index]=='getting':
                getting_index_list.append( index )
        return getting_index_list

    def get_block_worker():
        while True:
            if check_finished():
                return

            with block_list_write_lock:
                for index in range(len(block_list)):
                    if block_list[index]=='to_get':
                        block_list[index]='getting'
                        break
                else: # all is done or being done
                    index = random.choice( get_getting_index_list() )

            while True:
                try:
                    start = block_size * index
                    end = start + block_size - 1
                    if end > url_content_length:
                        end = url_content_length - 1
                    data = partial_get( args.url, start, end )
                    if len(data) != end - start + 1:  # restart ?
                        log( 'index {} data len wrong, theory:{}, real:{}'.format( index, end-start+1, len(data) ) )
                        with block_list_write_lock:
                            block_list[index] = 'to_get'
                    else:
                        with block_list_write_lock:
                            if block_list[index]=='getting': # this job may be done by another thread if thread is spare
                                block_list[index] = data
                        break
                except:
                    #traceback.print_exc()
                    sys.exc_clear()

    for worker_thread_index in range(worker_thread_count):
        worker_thread = threading.Thread( target=get_block_worker, args=[] )
        worker_thread.daemon = True
        worker_thread.start()

    while True:
        if check_finished():
            break
        else:
            time.sleep(3)

    with open(os.path.basename(args.output), 'wb' ) as f:
        for block in block_list:
            f.write( block )

if __name__=='__main__':
    global proxies

    argparser = argparse.ArgumentParser()
    argparser.add_argument('-u', '--url', type=str, required=True, help='url to download')
    argparser.add_argument('-o', '--output', type=str, required=True, help='output file name')
    argparser.add_argument('--proxy', type=str, required=False, help='proxy')
    args = argparser.parse_args()

    if args.proxy:
        proxies = {
            'http': args.proxy,
            'https': args.proxy,
        }
    
    wget( args )
